{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba33d3d-c17f-4019-94df-347bc4bb8c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p39/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b83b91d-f36a-4603-836d-dc6bc1df4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import random\n",
    "import os\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import pickle\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1478c9-3bbf-4826-b9a1-abc97cbe271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7ae67f-8168-4427-8ed4-fb86cc770430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MSCOCODataset(Dataset):\n",
    "    def __init__(self, args=None, basic_trans=None):\n",
    "        self.args = args\n",
    "        self.images = sorted(glob.glob(args.data_dir + '/*'))\n",
    "        self.ann_file = '{}/annotations/instances_{}.json'.format(args.ann_dir, args.mode)\n",
    "        self.coco = COCO(self.ann_file)\n",
    "        self.basic_trans = basic_trans\n",
    "        self.img_size = args.input_size\n",
    "        self.center = (self.img_size/2, self.img_size/2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)//4\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 363 triplets, 1089 images\n",
    "    \n",
    "        bg = self.getImage(self.images[idx*3])\n",
    "        fg = self.getImage(self.images[idx*3+1])\n",
    "        mask = self.getMask(self.images[idx*3+2])\n",
    "      \n",
    "        ##########\n",
    "\n",
    "        bg = self.basic_trans(image=bg)['image']\n",
    "        items = self.basic_trans(image=fg, mask=mask)\n",
    "        fg = items['image']\n",
    "        mask = items['mask'].unsqueeze(0)\n",
    "        \n",
    "        return bg, fg, mask>0.5\n",
    "    \n",
    "    def getClassName(self, classID, cats):\n",
    "        for i in range(len(cats)):\n",
    "            if cats[i]['id'] == classID:\n",
    "                return cats[i]['name']\n",
    "        return \"None\"\n",
    "    \n",
    "    def getImage(self, file_name):\n",
    "        img = cv2.imread(file_name, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        return img\n",
    "    \n",
    "    def getMask(self, file_name):\n",
    "        mask = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a72737-74a7-4729-a502-1f4896f465ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class GenCompModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GenCompModel, self).__init__()\n",
    "        self.args = args\n",
    "        self.img_size = args.input_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.stn = STN(7) # fg+bg+mask channels\n",
    "        self.colornet = LinearWithChannel(self.batch_size, self.img_size, self.img_size, 3) \n",
    "        self.refinenet = TransformNetwork(3, 3)\n",
    "    \n",
    "    def forward(self, fg, bg, mask):\n",
    "        HI, AI, trans_mat = self.stn(fg, bg, mask)\n",
    "        FI = self.colornet(HI) # changed to inputs as I\n",
    "        AI = AI > 0.5\n",
    "        R_in = torch.multiply(FI, AI) + torch.multiply(bg, ~AI)\n",
    "        R_out = self.refinenet(R_in)\n",
    "        return AI, HI, FI, trans_mat, R_in, R_out\n",
    "    \n",
    "class Discriminator(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super(Discriminator, self).__init__()\n",
    "        self.imgdisc = ImageDiscriminator(3, norm='spec')\n",
    "        self.segnet = TransformNetwork(3, 1)\n",
    "        \n",
    "    def forward(self, mask, R_out):\n",
    "        img_out = self.imgdisc(R_out)\n",
    "        fg_seg_out = self.segnet(torch.multiply(mask, R_out), last='sigmoid')\n",
    "        bg_seg_out = self.segnet(torch.multiply(~mask, R_out), last='sigmoid')\n",
    "\n",
    "        return img_out, fg_seg_out, bg_seg_out\n",
    "    \n",
    "class TransformNetwork(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):        \n",
    "        super(TransformNetwork, self).__init__()        \n",
    "        \n",
    "        self.layers = nn.Sequential(            \n",
    "            ConvLayer(in_ch, 32, 9, 1),\n",
    "            ConvLayer(32, 64, 3, 2),\n",
    "            ConvLayer(64, 128, 3, 2),\n",
    "            \n",
    "            ResidualLayer(128, 128, 3, 1),\n",
    "            ResidualLayer(128, 128, 3, 1),\n",
    "            ResidualLayer(128, 128, 3, 1),\n",
    "            ResidualLayer(128, 128, 3, 1),\n",
    "            ResidualLayer(128, 128, 3, 1),\n",
    "            \n",
    "            DeconvLayer(128, 64, 3, 1),\n",
    "            DeconvLayer(64, 32, 3, 1),\n",
    "            ConvLayer(32, out_ch, 9, 1, activation='linear'))\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, last=None):\n",
    "        x = self.layers(x)\n",
    "        if last:\n",
    "            x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class LinearWithChannel(nn.Module):\n",
    "    def __init__(self, batch_size, input_size, output_size, channel_size):\n",
    "        super(LinearWithChannel, self).__init__()\n",
    "        self.w = torch.nn.Parameter(torch.empty(channel_size))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(channel_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.w.view(1, 3, 1, 1) + self.b.view(1, 3, 1, 1)\n",
    "    \n",
    "class STN(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(STN, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        \n",
    "        # localization-network for STN\n",
    "        self.localization = nn.Sequential(\n",
    "            ConvLayer(self.in_ch, 16, 3, 1), # 256\n",
    "            ConvLayer(16, 16, 3, 1),\n",
    "            ConvLayer(16, 16*2, 3, 2), # 128\n",
    "            ConvLayer(16*2, 16*2, 3, 1),\n",
    "            ConvLayer(16*2, 16*4, 3, 2), # 64\n",
    "            ConvLayer(16*4, 16*4, 3, 1),\n",
    "            ConvLayer(16*4, 16*8, 3, 2), # 32\n",
    "            ConvLayer(16*8, 16*8, 3, 1), \n",
    "            ConvLayer(16*8, 16*16, 3, 2), # 16\n",
    "            ConvLayer(16*16, 16*16, 3, 1), \n",
    "            ConvLayer(16*16, 16*32, 3, 2), # 8\n",
    "            ConvLayer(16*32, 16*32, 3, 1), \n",
    "        )\n",
    "\n",
    "        # [3 * 2] 크기의 아핀(affine) 행렬에 대해 예측\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(16*32*8*8, 32*8*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32*8*8, 8*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(8*8, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, 2*3),\n",
    "        )\n",
    "\n",
    "        # 항등 변환(identity transformation)으로 가중치/바이어스 초기화\n",
    "        self.fc_loc[6].weight.data.zero_()\n",
    "        self.fc_loc[6].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # STN의 forward 함수\n",
    "    def stn(self, fg, bg, mask):\n",
    "        # x = x.type(torch.cuda.FloatTensor)\n",
    "        mask = mask.float()\n",
    "        inputs = torch.cat([fg, bg, mask], dim=1) #[B, (3+3+1), 256, 256]\n",
    "        xs = self.localization(inputs)\n",
    "        # print('xs shape:', xs.shape) #torch.Size([4, 10, 60, 60]) # [1, 128, 32, 32])\n",
    "        xs = xs.view(-1, 16*32*8*8) #xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, fg.size(), align_corners=False)\n",
    "        fg = F.grid_sample(fg, grid, align_corners=False)\n",
    "\n",
    "        grid = F.affine_grid(theta, mask.size(), align_corners=False)\n",
    "        mask = F.grid_sample(mask, grid, align_corners=False)\n",
    "        \n",
    "        # return fg, mask>0.5, theta\n",
    "        return fg, mask, theta\n",
    "\n",
    "    def forward(self, fg, bg, mask):\n",
    "        # 입력을 변환\n",
    "        fg, mask, trans_mat = self.stn(fg, bg, mask)\n",
    "        return fg, mask, trans_mat\n",
    "    \n",
    "class ConvLayer(nn.Module):    \n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, pad='reflect', activation='leaky', normalization='batch'):        \n",
    "        super(ConvLayer, self).__init__()\n",
    "        \n",
    "        # padding\n",
    "        if pad == 'reflect':            \n",
    "            self.pad = nn.ReflectionPad2d(kernel_size//2)\n",
    "        elif pad == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(kernel_size//2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not expected pad flag !!!\")\n",
    "    \n",
    "            \n",
    "        # convolution\n",
    "        self.conv_layer = nn.Conv2d(in_ch, out_ch, \n",
    "                                    kernel_size=kernel_size,\n",
    "                                    stride=stride)\n",
    "        if normalization == 'spec':\n",
    "            self.conv_layer = nn.utils.spectral_norm(self.conv_layer)\n",
    "           \n",
    "        \n",
    "        # activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()     \n",
    "        elif activation == 'leaky':\n",
    "            self.activation = nn.LeakyReLU(0.2)\n",
    "        elif activation == 'linear':\n",
    "            self.activation = lambda x : x\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not expected activation flag !!!\")\n",
    "\n",
    "        # normalization \n",
    "        if normalization == 'instance':            \n",
    "            self.normalization = nn.InstanceNorm2d(out_ch, affine=True)\n",
    "        elif normalization == 'batch':\n",
    "            self.normalization = nn.BatchNorm2d(out_ch, affine=True)\n",
    "        elif normalization == 'spec':\n",
    "            self.normalization = None\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not expected normalization flag !!!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.conv_layer(x)\n",
    "        if self.normalization:\n",
    "            x = self.normalization(x)\n",
    "        x = self.activation(x)        \n",
    "        return x\n",
    "    \n",
    "class ResidualLayer(nn.Module):    \n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, pad='reflect', normalization='batch'):        \n",
    "        super(ResidualLayer, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvLayer(in_ch, out_ch, kernel_size, stride, pad, \n",
    "                               activation='relu', \n",
    "                               normalization=normalization)\n",
    "        \n",
    "        self.conv2 = ConvLayer(out_ch, out_ch, kernel_size, stride, pad, \n",
    "                               activation='linear', \n",
    "                               normalization=normalization)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        return self.conv2(y) + x\n",
    "    \n",
    "class DeconvLayer(nn.Module):    \n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, pad='reflect', activation='leaky', normalization='batch', upsample='nearest'):        \n",
    "        super(DeconvLayer, self).__init__()\n",
    "        \n",
    "        # upsample\n",
    "        self.upsample = upsample\n",
    "        \n",
    "        # pad\n",
    "        if pad == 'reflect':            \n",
    "            self.pad = nn.ReflectionPad2d(kernel_size//2)\n",
    "        elif pad == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(kernel_size//2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not expected pad flag !!!\")        \n",
    "        \n",
    "        # conv\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride)\n",
    "        \n",
    "        # activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leaky':\n",
    "            self.activation = nn.LeakyReLU(0.2)\n",
    "        elif activation == 'linear':\n",
    "            self.activation = lambda x : x\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not expected activation flag !!!\")\n",
    "        \n",
    "        # normalization\n",
    "        if normalization == 'instance':\n",
    "            self.normalization = nn.InstanceNorm2d(out_ch, affine=True)\n",
    "        elif normalization == 'batch':\n",
    "            self.normalization = nn.BatchNorm2d(out_ch, affine=True)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not expected normalization flag !!!\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode=self.upsample)        \n",
    "        x = self.pad(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.normalization(x)        \n",
    "        x = self.activation(x)        \n",
    "        return x\n",
    "    \n",
    "class ImageDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch, norm='spec'):\n",
    "        super(ImageDiscriminator, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            ConvLayer(in_ch, 64, 10, 4, normalization=norm, activation='leaky'), #256->64\n",
    "            ConvLayer(64, 128, 10, 4, normalization=norm, activation='leaky'), #64->16\n",
    "            ConvLayer(128, 256, 10, 4, normalization=norm, activation='leaky'), #16->4\n",
    "            nn.Conv2d(256, 1, 5, 1), #4->1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98721815-825f-4c38-ab82-cf7bfad03c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    ann_dir = '/home/ubuntu/COCOdataset2017',\n",
    "    data_dir = '/home/ubuntu/GCCdataset/val_bg',\n",
    "    save_model_dir = '/home/ubuntu/GCC-GAN-server/models/',\n",
    "    mode = 'val', # or 'test'\n",
    "    batch_size = 32,\n",
    "    input_size = 256,\n",
    "    # epochs = 8,\n",
    "    lr = 2e-5,\n",
    "    beta = 0.5,\n",
    "    test_interval = 50,\n",
    "    device_id = 6,#[0, 1, 2, 3]\n",
    "    vis_id = 'test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4832f60e-6d2a-463e-9827-d263ebcd4bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:6\n",
      "# Workers: 8\n",
      "loading annotations into memory...\n",
      "Done (t=0.88s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "device = 'cuda:'+str(args.device_id) if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Device:', device)\n",
    "\n",
    "basic_transform = A.Compose([\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],),\n",
    "        ToTensorV2(),\n",
    "])\n",
    "affine_transform = A.IAAAffine(scale=(0.8, 1.2), translate_percent=(0.2, 0.4), rotate=(-10, 10), shear=15, mode='constant')\n",
    "\n",
    "\n",
    "num_workers = 8 # 4 * len(args.device_ids)\n",
    "print('# Workers:', num_workers)\n",
    "\n",
    "dataset = MSCOCODataset(args, basic_trans=basic_transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb52ac8-47b4-4db3-83a1-e03be2693ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from torch.optim import Adam\n",
    "import visdom\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def denorm(tensor):\n",
    "    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n",
    "    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n",
    "    res = torch.clamp(tensor * std + mean, 0, 1)\n",
    "    return res\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    image_numpy = image_tensor.numpy() #image_tensor[0].permute(1,2,0).detach().cpu().float().numpy()\n",
    "    # image_numpy = image_numpy * 0.5 - 0.5\n",
    "    image_numpy = image_numpy * 255\n",
    "    # image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    # image_numpy = (image_numpy + 1) / 2.0 * 255.0\n",
    "    image_numpy = np.clip(image_numpy, 0, 255)\n",
    "\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Conv2d):\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#     elif isinstance(m, nn.BatchNorm2d):\n",
    "#         nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "# def save_checkpoint(epoch, model, optimizer, filename):\n",
    "#     state = {\n",
    "#         'Epoch': epoch,\n",
    "#         'State_dict': model.state_dict(),\n",
    "#         'optimizer': optimizer.state_dict()\n",
    "#     }\n",
    "#     torch.save(state, filename)\n",
    "    \n",
    "def load_checkpoint(filename):\n",
    "    state = torch.load(filename, map_location='cpu')\n",
    "    return state['State_dict'], state['optimizer']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Visdom display initialization\n",
    "    vis = visdom.Visdom(env=args.vis_id)\n",
    "    vis.close(env=args.vis_id)\n",
    "    \n",
    "    win_ids = []\n",
    "\n",
    "    for i in range(args.batch_size):\n",
    "        win_ids.append(vis.images(np.random.rand(7, 3, 256, 256), env=args.vis_id))\n",
    "    \n",
    "    net_state, opt_state = load_checkpoint(args.save_model_dir + 'netHCRS-free3.pth')\n",
    "    disc_state, optD_state = load_checkpoint(args.save_model_dir + 'discHCRS-free3.pth')\n",
    "    \n",
    "    net = GenCompModel(args) # generator\n",
    "    # net.apply(weights_init)\n",
    "    net.load_state_dict(net_state)\n",
    "\n",
    "    disc = Discriminator()\n",
    "    # disc.apply(weights_init)\n",
    "    disc.load_state_dict(disc_state)\n",
    "\n",
    "    net.to(device)\n",
    "    disc.to(device)\n",
    "    \n",
    "    net.eval()\n",
    "    disc.eval()\n",
    "\n",
    "    real_label = 1.\n",
    "    fake_label = 0.\n",
    "\n",
    "    optimizer = Adam(net.parameters(), lr=args.lr, betas=(args.beta, 0.999))\n",
    "    optimizerD = Adam(disc.parameters(), lr=args.lr, betas=(args.beta, 0.999))\n",
    "    \n",
    "    optimizer.load_state_dict(opt_state)\n",
    "    optimizerD.load_state_dict(optD_state)\n",
    "        \n",
    "\n",
    "    SMOOTH = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8179ed70-86e5-4dca-9bb7-a640c091d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005987584590911865\n",
      "Inference Time Taken: 0.0006 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'window_3b34e2950d3c64'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i, items in enumerate(dataloader):\n",
    "    bg = bg.unsqueeze(0).to(device); fg = fg.unsqueeze(0).to(device); mask = mask.unsqueeze(0).to(device) #bg = items[0].to(device); fg = items[1].to(device); mask = items[2].to(device)\n",
    "    # print(bg.shape, fg.shape, mask.shape)\n",
    "    infer_time_total = 0\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        AI, HI, FI, pred_mat, R_in, R_out = net(fg, bg, mask)\n",
    "        R_img_out, R_fg_seg, R_bg_seg = disc(AI, R_out)\n",
    "        temp_time = time.time() - inference_start_time\n",
    "        infer_time_total += temp_time\n",
    "        print(temp_time/args.batch_size)\n",
    "            \n",
    "    print('Inference Time Taken: %.4f sec' % (infer_time_total/i/args.batch_size)) #divide by #batch and #img in batch\n",
    "\n",
    "    inputs = tensor2im(denorm((torch.multiply(fg, mask) + torch.multiply(bg, ~mask)).detach().cpu()).float())\n",
    "    FI_out = tensor2im(denorm(FI.detach().cpu()).float())\n",
    "    HI_out = tensor2im(denorm(HI.detach().cpu()).float())\n",
    "    AI_out = tensor2im(AI.detach().cpu())\n",
    "    R_in = tensor2im(denorm(R_in.detach().cpu()).float())\n",
    "    seg_out = tensor2im((R_fg_seg+R_bg_seg).detach().cpu())\n",
    "    R_out = tensor2im(denorm(R_out.detach().cpu()).float())\n",
    "\n",
    "    AI_out = np.concatenate([AI_out, AI_out, AI_out], axis=1)\n",
    "    seg_out = np.concatenate([seg_out, seg_out, seg_out], axis=1)\n",
    "\n",
    "    total = np.concatenate([inputs, FI_out, HI_out, AI_out, R_in, seg_out, R_out], axis=1)\n",
    "\n",
    "    for i in range(args.batch_size):\n",
    "        batch = total[i, :, :, :] # (17, 256, 256)\n",
    "        batch_out = np.concatenate([np.expand_dims(batch[:3, :, :], axis=0), np.expand_dims(batch[3:6, :, :], axis=0), np.expand_dims(batch[6:9, :, :], axis=0), np.expand_dims(batch[9:12, :, :], axis=0),\\\n",
    "                                    np.expand_dims(batch[12:15, :, :], axis=0), np.expand_dims(batch[15:18, :, :], axis=0), np.expand_dims(batch[18:21, :, :], axis=0)], axis=0)\n",
    "        # batch_out = np.concatenate([batch[:, :3, :, :], batch[:, 3:6, :, :], batch[:, 6:9, :, :], batch[:, 9:10, :, :], batch[:, 10:13, :, :], batch[:, 13:14, :, :], batch[:, 14:17, :, :]], axis=0)\n",
    "        vis.images(batch_out, win=win_ids[i], opts=dict(title=str(i)), env=args.vis_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b10771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image testing\n",
    "\n",
    "import time\n",
    "\n",
    "bg = cv2.imread(\"/home/ubuntu/RealIndoors/automatic/bg.jpeg\", cv2.IMREAD_COLOR)\n",
    "bg = cv2.cvtColor(bg, cv2.COLOR_BGR2RGB)\n",
    "bg = cv2.resize(bg, (256, 256))\n",
    "fg = cv2.imread(\"/home/ubuntu/RealIndoors/automatic/fg.jpeg\", cv2.IMREAD_COLOR)\n",
    "fg = cv2.cvtColor(fg, cv2.COLOR_BGR2RGB)\n",
    "fg = cv2.resize(fg, (256, 256))\n",
    "mask = cv2.imread(\"/home/ubuntu/RealIndoors/automatic/mask.png\", cv2.IMREAD_GRAYSCALE)\n",
    "mask = cv2.resize(mask, (256, 256))\n",
    "\n",
    "bg = basic_transform(image=bg)['image']\n",
    "mask = mask>0.5\n",
    "items = affine_transform(image=fg, mask=mask)\n",
    "items = basic_transform(image=items['image'], mask=items['mask'])\n",
    "fg = items['image']\n",
    "mask = items['mask'].unsqueeze(0)\n",
    "\n",
    "bg = bg.unsqueeze(0).to(device); fg = fg.unsqueeze(0).to(device); mask = mask.unsqueeze(0).to(device)\n",
    "\n",
    "infer_time_total = 0\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    AI, HI, FI, pred_mat, R_in, R_out = net(fg, bg, mask)\n",
    "    R_img_out, R_fg_seg, R_bg_seg = disc(AI, R_out)\n",
    "    temp_time = time.time() - inference_start_time\n",
    "    infer_time_total += temp_time\n",
    "    print(temp_time/args.batch_size)\n",
    "        \n",
    "print('Inference Time Taken: %.4f sec' % (infer_time_total/i/args.batch_size)) #divide by #batch and #img in batch\n",
    "\n",
    "\n",
    "inputs = tensor2im(denorm((torch.multiply(fg, mask) + torch.multiply(bg, ~mask)).detach().cpu()).float())\n",
    "\n",
    "FI_out = tensor2im(denorm(FI.detach().cpu()).float())\n",
    "HI_out = tensor2im(denorm(HI.detach().cpu()).float())\n",
    "AI_out = tensor2im(AI.detach().cpu())\n",
    "R_in = tensor2im(denorm(R_in.detach().cpu()).float())\n",
    "seg_out = tensor2im((R_fg_seg+R_bg_seg).detach().cpu())\n",
    "R_out = tensor2im(denorm(R_out.detach().cpu()).float())\n",
    "\n",
    "AI_out = np.concatenate([AI_out, AI_out, AI_out], axis=1)\n",
    "seg_out = np.concatenate([seg_out, seg_out, seg_out], axis=1)\n",
    "\n",
    "total = np.concatenate([inputs, FI_out, HI_out, AI_out, R_in, seg_out, R_out], axis=1)\n",
    "\n",
    "for i in range(args.batch_size):\n",
    "    batch = total[i, :, :, :] # (17, 256, 256)\n",
    "    batch_out = np.concatenate([np.expand_dims(batch[:3, :, :], axis=0), np.expand_dims(batch[3:6, :, :], axis=0), np.expand_dims(batch[6:9, :, :], axis=0), np.expand_dims(batch[9:12, :, :], axis=0),\\\n",
    "                                np.expand_dims(batch[12:15, :, :], axis=0), np.expand_dims(batch[15:18, :, :], axis=0), np.expand_dims(batch[18:21, :, :], axis=0)], axis=0)\n",
    "    vis.images(batch_out, win=win_ids[i], opts=dict(title=str(i)), env=args.vis_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb71ac050f92a5d5e3cdea462f22e174d379ee0836c3076b1e7df4a375a19e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
